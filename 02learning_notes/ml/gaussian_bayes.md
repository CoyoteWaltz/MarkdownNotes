# 贝叶斯决策

---

## 算法原理(非常简单的说明一下)

贝叶斯公式是概率论课中学过的，如此简单的小公式，竟然发挥着巨大的决策作用(暖个场)。

决策的目的: **分类**

主要思想是: 计算样本对于每一个类别的后验概率$P(y_i|x)$，这是一个条件概率，好比拿到一个样本，他是第 i 类的可能性的大小，如何决策，一般做法就是将使得这个后验概率最大的类别作为这个未知样本的类别。

贝叶斯公式:

$$
P(y_c|x) = \frac{P(y_c, x)}{P(x)} = \frac{P(x|y_c)P(y_c)}{\sum_j p(x|y_j)p(y_j)}
$$

非常亲切的公式，这里不多解释。主要说一下分子上的条件概率，可以看成是在某一类别中，x 随机变量取值的概率，举个例子，好西瓜中绿色西瓜的概率是 0.7，坏西瓜中绿色西瓜的概率是 0.1，当然这是一维的例子，如果随机变量的维度多了，那就要用概率密度乘法表示$p(\mathrm{x}|y_c) = p(x_1|y_c) p(x_2|y_c)...p(x_i|y_c) $这里用小写 p 表示概率密度。

除了最后要计算的后验概率，其他的都是我们拿到数据集之后可以得到的先验概率(empirical probability)，或者是人为设定的概率密度函数。

其实直觉上来说有了所有类别的后验概率之后就会取其中最大的那个，殊不知这原来也是有依据的

#### 最小错误率决策

拿二分类举例子，原本是 1 类的样本，预测成了 0 类，此时预测是错的，错误的概率看成是$P(y_0|x)$

我计算出 0 类的$P(y_0|x)$更大

即最大后验概率决策

#### 最小风险决策

这个风险是指将某一个类别(真实的)分错成另一个类所需在真实世界中的代价。

https://github.com/tushushu/imylu

## 提升改进

### 用 log probability 替代原来的概率值

由于在计算贝叶斯公式分母上的条件概率$p(y|\mathbf{x}) = \prod_i^n p(y|x_i)$的时候，当特征数量 n 很大的时候，每个特征分量上的概率值(0-1 之间)累乘之后的结果会趋向于 0，越乘越小会引起下溢(在 python 中无法表示，据说 c++不会出现这样的问题?)导致结果不精确。

一个最常见的解决方法就是用对数概率去表示计算概率的公式，原因如下

- 概率值在 0-1 之间，那么取了 log 之后的值会在$(-\infin, 0]$之间，这个范围比 0-1 大很多，可以提升精度
- 不等式符号不变性
- log(a\*b) = log a + log b 不多说了

公式推导

$$
\begin{aligned}
	\hat{y} =& \mathop{\arg\max}_{c \in class}& [\prod_{j=1}^nP(x_i|y_c)]P(y_c) \\
			=& \mathop{\arg\max}_{c \in class}& \ln \left([\prod_{j=1}^nP(x_i|y_c)]P(y_c) \right) \\
			=& \mathop{\arg\max}_{c \in class}& \ln [\prod_{j=1}^nP(x_i|y_c)] + \ln P(y_c)  \\
			=& \mathop{\arg\max}_{c \in class}& \sum_{i=1}^n\ln P(x_i|y_c) + \ln P(y_c)
\end{aligned}
$$

所以在代码中用这个公式就可以了

注意的点:

- 计算的都是加和
- 加和的值都是负数，一个更负的数相当于是更小的数
- 特征数小的时候效果几乎和常规方法一样

### 用不同的概率密度函数
